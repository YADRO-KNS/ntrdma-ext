From: Dave Jiang <dave.jiang@intel.com>
Date: Mon, 27 Jul 2015 05:51:59 -0400
Subject: [PATCH 2/2] ioatdma: Add immediate data op

Implementing the operation to allow immediate data DMA. We will use the
BLOCKFILL operation for ioatdma to implement this. This will allow us to
DMA up to 8 bytes of immediate data.

Signed-off-by: Dave Jiang <dave.jiang@intel.com>
Signed-off-by: Allen Hubbe <Allen.Hubbe@emc.com>
---
 drivers/dma/ioat/dma.h  |  3 +++
 drivers/dma/ioat/hw.h   |  1 +
 drivers/dma/ioat/init.c |  5 +++++
 drivers/dma/ioat/prep.c | 40 ++++++++++++++++++++++++++++++++++++++++
 4 files changed, 49 insertions(+)

diff --git a/drivers/dma/ioat/dma.h b/drivers/dma/ioat/dma.h
index 1bc084986646..e470abc8e450 100644
--- a/drivers/dma/ioat/dma.h
+++ b/drivers/dma/ioat/dma.h
@@ -390,6 +390,9 @@ struct dma_async_tx_descriptor *
 ioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,
 			   dma_addr_t dma_src, size_t len, unsigned long flags);
 struct dma_async_tx_descriptor *
+ioat_dma_prep_imm_lock(struct dma_chan *c, dma_addr_t dma_dest,
+		       void *src, size_t len, unsigned long flags);
+struct dma_async_tx_descriptor *
 ioat_prep_interrupt_lock(struct dma_chan *c, unsigned long flags);
 struct dma_async_tx_descriptor *
 ioat_prep_xor(struct dma_chan *chan, dma_addr_t dest, dma_addr_t *src,
diff --git a/drivers/dma/ioat/hw.h b/drivers/dma/ioat/hw.h
index ec64aced5655..29cdd13b3d7d 100644
--- a/drivers/dma/ioat/hw.h
+++ b/drivers/dma/ioat/hw.h
@@ -80,6 +80,7 @@ struct ioat_dma_descriptor {
 			unsigned int hint:1;
 			unsigned int rsvd2:13;
 			#define IOAT_OP_COPY 0x00
+			#define IOAT_OP_BLOCKFILL 0x01
 			unsigned int op:8;
 		} ctl_f;
 	};
diff --git a/drivers/dma/ioat/init.c b/drivers/dma/ioat/init.c
index 60a7c3211e0d..e344c8ca0b5f 100644
--- a/drivers/dma/ioat/init.c
+++ b/drivers/dma/ioat/init.c
@@ -1066,6 +1066,11 @@ static int ioat3_dma_probe(struct ioatdma_device *ioat_dma, int dca)
 
 	ioat_dma->cap = readl(ioat_dma->reg_base + IOAT_DMA_CAP_OFFSET);
 
+	if (ioat_dma->cap & IOAT_CAP_FILL_BLOCK) {
+		dma->device_prep_dma_imm = ioat_dma_prep_imm_lock;
+		dma->imm_max_size = 8;
+	}
+
 	if (is_xeon_cb32(pdev) || is_bwd_noraid(pdev))
 		ioat_dma->cap &=
 			~(IOAT_CAP_XOR | IOAT_CAP_PQ | IOAT_CAP_RAID16SS);
diff --git a/drivers/dma/ioat/prep.c b/drivers/dma/ioat/prep.c
index e323a4036908..d9956dc96242 100644
--- a/drivers/dma/ioat/prep.c
+++ b/drivers/dma/ioat/prep.c
@@ -154,6 +154,46 @@ ioat_dma_prep_memcpy_lock(struct dma_chan *c, dma_addr_t dma_dest,
 	return &desc->txd;
 }
 
+struct dma_async_tx_descriptor *
+ioat_dma_prep_imm_lock(struct dma_chan *c, dma_addr_t dma_dest,
+		       void *src, size_t len, unsigned long flags)
+{
+	struct ioatdma_chan *ioat_chan = to_ioat_chan(c);
+	struct ioatdma_device *ioat_dma = ioat_chan->ioat_dma;
+	struct dma_device *dma = &ioat_dma->dma_dev;
+	struct ioat_dma_descriptor *hw;
+	struct ioat_ring_ent *desc;
+	size_t total_len = len;
+	int num_descs, idx, i;
+
+	if (len > dma->imm_max_size)
+		return NULL;
+
+	num_descs = 1;
+	if (ioat_check_space_lock(ioat_chan, num_descs) == 0)
+		idx = ioat_chan->head;
+	else
+		return NULL;
+	i = 0;
+
+	desc = ioat_get_ring_ent(ioat_chan, idx + i);
+	hw = desc->hw;
+
+	hw->size = len;
+	hw->ctl = 0;
+	hw->ctl_f.op = IOAT_OP_BLOCKFILL;
+	hw->src_addr = *(u64 *)src;
+	hw->dst_addr = dma_dest;
+	desc->txd.flags = flags;
+	desc->len = total_len;
+	hw->ctl_f.int_en = !!(flags & DMA_PREP_INTERRUPT);
+	hw->ctl_f.fence = !!(flags & DMA_PREP_FENCE);
+	hw->ctl_f.compl_write = 1;
+	dump_desc_dbg(ioat_chan, desc);
+	/* we leave the channel locked to ensure in order submission */
+
+	return &desc->txd;
+}
 
 static struct dma_async_tx_descriptor *
 __ioat_prep_xor_lock(struct dma_chan *c, enum sum_check_flags *result,
-- 
2.5.0.rc1

